{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 by Josh Thyng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Object Size Measurement\n",
    "\n",
    "For this task, work on “book.jpg” to measure the size of a book placed on an A4 paper\n",
    "with dimensions 27.8cm by 21.5cm.\n",
    "\n",
    "1. Detection and Measurement: Utilize appropriate techniques to detect the book in\n",
    "   the image and measure its dimensions (height and width in centimeters). Describe\n",
    "   the methodology and rationale for your approach in your report.\n",
    "2. Annotation: After detecting the book, annotate the image by drawing a rectangle\n",
    "   around the book. Clearly display the measured width and height on the image.\n",
    "3. Comparison and Documentation: The actual size of the book is 8 cm by 10.6 cm.\n",
    "   Compare your measurements to these dimensions and document the results in\n",
    "   your report. Aim for an error rate of less than 10% with the ideal method.\n",
    "   Ensure that your report is detailed, including descriptions of the methods used, your\n",
    "   results (screenshots), and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write-Up\n",
    "\n",
    "This project is meant to detect the size of a book placed on an A4 sheet of paper, outlining the book with a rectangle, and calculating its real-world dimensions (8cm x 10.6cm). The process was very difficult and took a couple steps including contour detection, _pixel-to-centimeter conversion_, and ordering of the book's corner points. Below is an image showing the final result:\n",
    "\n",
    "![Book Detection Results](book_results.png)\n",
    "\n",
    "## Challenges and Solutions\n",
    "\n",
    "### 1. Contour Detection\n",
    "\n",
    "Detecting the contours of the book was one of the most difficult parts of the project. Noise, lighting conditions, and irregularities in the image made it challenging to isolate the book's outline.\n",
    "\n",
    "- **How I Did It**: I converted the image to the HSV color space for better color segmentation, created a mask based on the color of the book, and applied [morphological operations](https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html) to clean up the mask.\n",
    "\n",
    "### 2. Outlining the Book\n",
    "\n",
    "After detecting the contours, outlining the book with a rectangle in my opinion seemed like the easiest way to help get the actual size.\n",
    "\n",
    "- **How I Did It**: I used the `cv2.minAreaRect()` function to handle rotated objects and extract the smallest rectangle around the book. The rectangle was then drawn using the corner points calculated by `cv2.boxPoints()`. This was not to bad.\n",
    "\n",
    "### 3. Converting Size to Pixels\n",
    "\n",
    "Once the book was outlined, calculating its dimensions in pixels was the next challenge. Because the book could be tilted, the measurement needed to account for any angle.\n",
    "\n",
    "- **How I Did it**: I used the Euclidean distance between opposite corners of the rectangle to calculate the width and height of the book in pixels.\n",
    "\n",
    "### 4. The `order_points()` Function\n",
    "\n",
    "The detected corner points were not always ordered in a consistent way, which complicated this a lot more.\n",
    "\n",
    "- **How I Did It**: I wrote the `order_points()` function to reorder the four points into a predictable sequence: top-left, top-right, bottom-right, and bottom-left. This let me make sure it was reliable when computing the book's dimensions.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This kind-of (not 100% accurately) detected and measured the dimensions of a book placed on an A4 sheet of paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)  # reads in the 'book.jpg'\n",
    "    hsv = cv2.cvtColor(\n",
    "        img, cv2.COLOR_BGR2HSV\n",
    "    )  # this just converts the image from BGR to HSV, this will help isolate the book\n",
    "    return img, hsv\n",
    "\n",
    "\n",
    "def create_mask(hsv, lower_color, upper_color, kernel_size=(7, 7)):\n",
    "    mask = cv2.inRange(\n",
    "        hsv, lower_color, upper_color\n",
    "    )  # this creates us a binary image where if a pixel is in our color range they are white and if not they are black (from HSV value)\n",
    "    kernel = np.ones(\n",
    "        kernel_size, np.uint8\n",
    "    )  # will be used later to perform transformations\n",
    "    mask = cv2.morphologyEx(\n",
    "        mask, cv2.MORPH_CLOSE, kernel\n",
    "    )  # this makes our mask even clearer by filling in black holes if there are any\n",
    "    mask = cv2.dilate(\n",
    "        mask, kernel, iterations=1\n",
    "    )  # this isn't needed but makes the area larger making contours more complete essentially\n",
    "    return mask\n",
    "\n",
    "\n",
    "def find_largest_contour(mask, min_area=5000):\n",
    "    contours, _ = cv2.findContours(\n",
    "        mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )  # this just gets us the boundaries of the white regions in the binary mask (what we did in previous function)\n",
    "    large_contours = [\n",
    "        cnt for cnt in contours if cv2.contourArea(cnt) > min_area\n",
    "    ]  # just filters out smaller contours\n",
    "    return (\n",
    "        max(large_contours, key=cv2.contourArea) if large_contours else None\n",
    "    )  # returns the contour with the largest area from filtered list\n",
    "\n",
    "\n",
    "# function finds smallest rectangle even if rotated that encloses the objected represented by contour and draws it around the image.\n",
    "def draw_rectangle(img, contour, color=(0, 0, 255), thickness=2):\n",
    "    rect = cv2.minAreaRect(contour)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    cv2.drawContours(img, [box], 0, color, thickness)\n",
    "    return box\n",
    "\n",
    "\n",
    "# simply put this function will take in the unorder points of the four corners and then put them in order\n",
    "# this is done with top-left, top-right, bottom-left, and bottom-right.\n",
    "def order_points(pts):\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "    s = pts.sum(axis=1)\n",
    "    diff = np.diff(pts, axis=1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    return rect\n",
    "\n",
    "\n",
    "# this function will just calcualte the Uuclidean distance between the corners, then averages the distances to account for any irregualrites in the measurement\n",
    "# can happen if distored or rotated\n",
    "def compute_dimensions(ordered_pts):\n",
    "    (tl, tr, br, bl) = (\n",
    "        ordered_pts  # unpacks the ordered points into four seperate variables, representing four corners.\n",
    "    )\n",
    "    widthA = np.linalg.norm(br - bl)\n",
    "    widthB = np.linalg.norm(tr - tl)\n",
    "    heightA = np.linalg.norm(tr - br)\n",
    "    heightB = np.linalg.norm(tl - bl)\n",
    "    return (widthA + widthB) / 2.0, (heightA + heightB) / 2.0\n",
    "\n",
    "\n",
    "# this function is used to get the scale of the image by calculating the number of pixels per centimeter.\n",
    "def get_paper_dimensions(image, known_dims_cm=(21.5, 27.8)):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "    contours, _ = cv2.findContours(\n",
    "        edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )  # finds contours in the binary edge-detected image\n",
    "\n",
    "    max_area, paper_contour = 0, None\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        peri = cv2.arcLength(cnt, True)\n",
    "        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
    "        if len(approx) == 4 and area > max_area:\n",
    "            paper_contour, max_area = approx, area\n",
    "\n",
    "    if paper_contour is None:\n",
    "        print(\"no paper contour found\")\n",
    "        exit()\n",
    "\n",
    "    ordered_pts = order_points(paper_contour.reshape(4, 2))\n",
    "    width_px, height_px = compute_dimensions(ordered_pts)\n",
    "\n",
    "    # compute pixels per cm\n",
    "    pixels_per_cm = (\n",
    "        (width_px / known_dims_cm[0]) + (height_px / known_dims_cm[1])\n",
    "    ) / 2.0\n",
    "    return pixels_per_cm\n",
    "\n",
    "\n",
    "def detect_book_size(img, hsv, pixels_per_cm):\n",
    "    # define color range for navy blue book cover\n",
    "    lower_blue = np.array([90, 50, 20])\n",
    "    upper_blue = np.array([130, 255, 255])\n",
    "\n",
    "    mask = create_mask(hsv, lower_blue, upper_blue)\n",
    "    book_contour = find_largest_contour(mask)\n",
    "\n",
    "    if book_contour is not None:\n",
    "        box = draw_rectangle(img, book_contour)\n",
    "        ordered_box = order_points(box)\n",
    "        book_width_px, book_height_px = compute_dimensions(ordered_box)\n",
    "\n",
    "        # dalculate real-world book dimensions\n",
    "        width_cm = round(book_width_px / pixels_per_cm, 1)\n",
    "        height_cm = round(book_height_px / pixels_per_cm, 1)\n",
    "\n",
    "        # display book dimensions on the image\n",
    "        size_text = f\"{width_cm} cm x {height_cm} cm\"\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            size_text,\n",
    "            (int(ordered_box[0][1]), int(ordered_box[0][1] - 250)),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            1,\n",
    "            (0, 255, 255),\n",
    "            2,\n",
    "        )\n",
    "    else:\n",
    "        print(\"no contours found for the book\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    img, hsv = load_and_preprocess_image(\"book.jpg\")\n",
    "\n",
    "    # Get paper's pixels per cm ratio\n",
    "    pixels_per_cm = get_paper_dimensions(img.copy())\n",
    "\n",
    "    # Detect and measure the book on the paper\n",
    "    detect_book_size(img, hsv, pixels_per_cm)\n",
    "\n",
    "    # Display the final image with book size\n",
    "    cv2.imshow(\"book with size\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text Recognition\n",
    "\n",
    "## Write-Up\n",
    "\n",
    "The goal was to detect and recognize text on traffic signs, but the OCR system did not succeed in producing accurate results for all signs.\n",
    "\n",
    "![Sign](test_results.png)\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "One of the main takeaways from this project was the complexity of using computer vision techniques to detect text. Although the model failed to recognize text in all images, **using Matplotlib** to visualize the image processing stages in real time was pretty helpful in understanding how to improve each case.\n",
    "\n",
    "While I could not achieve high accuracy, Matplotlib helped me analyze how preprocessing affected the input images and showed what went wrong with certain transformations.\n",
    "\n",
    "## Methods Used\n",
    "\n",
    "### 1. **Preprocessing Techniques**\n",
    "\n",
    "The first step to improving OCR accuracy was to preprocess the images:\n",
    "\n",
    "- **Grayscale Conversion:** Simplified the images by reducing color variations.\n",
    "- **Gaussian Blurring:** Reduced noise while preserving the edges of text characters.\n",
    "- **Histogram Equalization:** Improved contrast between text and the background.\n",
    "- **Adaptive Thresholding:** Improved binarization, particularly under uneven lighting conditions.\n",
    "- **Morphological Transformations:** Removed small noise and filled gaps in text characters.\n",
    "- **Image Resizing:** Enlarged the image to increase OCR performance on small text.\n",
    "- **Sharpening:** Applied to improve the edges of text characters.\n",
    "\n",
    "### 2. **Tesseract OCR Engine Configurations**\n",
    "\n",
    "- **PSM (Page Segmentation Mode):** Depending on the expected layout of the text, different PSM modes were applied:\n",
    "  - `--psm 8`: Treats the image as a single word.\n",
    "  - `--psm 6`: Assumes a single uniform block of text.\n",
    "- **OEM (OCR Engine Mode):** The LSTM OCR engine (`--oem 1`) was used for better recognition through deep learning models (_i don't think it worked for me -\\_-_)\n",
    "\n",
    "### 3. **Matplotlib for Testing and Debugging**\n",
    "\n",
    "The most important factor in improving the model was using **Matplotlib** to display the images at various stages of preprocessing. By visualizing the changes applied at each step (blurring, thresholding, morphological operations), I got insight into how the changes were affecting the recognition results.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Improving OCR accuracy for traffic sign recognition was a super hard. Some preprocessing techniques, like adaptive thresholding and morphological transformations, helped improve text detection, the task was complicated by the variations in image quality, text size, and environmental conditions.\n",
    "\n",
    "So in conclusion I should have used a different model for each grapic instead of using one for each image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Text: DRIVE CAREFULLY\n",
      "Recognized Text: DRIVE\n",
      "CAREFULLY\n",
      "Accuracy: 93.33%\n",
      "\n",
      "Expected Text: AHEAD\n",
      "Recognized Text: BO\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Expected Text: UTILITY WORK AHEAD\n",
      "Recognized Text: SO\n",
      "OP 4\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Expected Text: NO PASSING ZONE\n",
      "Recognized Text: NQ\n",
      "PASSING\n",
      "ZONE\n",
      "Accuracy: 80.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    filtered = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "\n",
    "    _, thresh = cv2.threshold(filtered, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "    resized = cv2.resize(morph, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    kernel_sharpening = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    cleaned_up = cv2.filter2D(resized, -1, kernel_sharpening)\n",
    "\n",
    "    return cleaned_up\n",
    "\n",
    "\n",
    "def extract_text(image, psm=6):\n",
    "\n",
    "    custom_config = r\"--oem 3 --psm {} -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\".format(\n",
    "        psm\n",
    "    )\n",
    "    text = pytesseract.image_to_string(image, config=custom_config)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def calculate_accuracy(expected_text, recognized_text):\n",
    "    expected_text = expected_text.lower()\n",
    "    recognized_text = recognized_text.lower()\n",
    "\n",
    "    correct_chars = sum(1 for e, r in zip(expected_text, recognized_text) if e == r)\n",
    "    total_chars = len(expected_text)\n",
    "\n",
    "    return correct_chars / total_chars * 100 if total_chars > 0 else 0\n",
    "\n",
    "\n",
    "def process_sign_image(image_path, expected_text):\n",
    "    processed_img = preprocess_image(image_path)\n",
    "    if len(expected_text.split()) <= 1:\n",
    "        psm = 8\n",
    "    else:\n",
    "        psm = 6\n",
    "\n",
    "    recognized_text = extract_text(processed_img, psm=psm)\n",
    "    accuracy = calculate_accuracy(expected_text, recognized_text)\n",
    "\n",
    "    print(f\"Expected Text: {expected_text}\")\n",
    "    print(f\"Recognized Text: {recognized_text}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "    return recognized_text, accuracy\n",
    "\n",
    "\n",
    "sign_images = [\n",
    "    (\"sign1.jpg\", \"DRIVE CAREFULLY\"),\n",
    "    (\"sign2.jpg\", \"AHEAD\"),\n",
    "    (\"sign3.jpg\", \"UTILITY WORK AHEAD\"),\n",
    "    (\"sign4.jpg\", \"NO PASSING ZONE\"),\n",
    "]\n",
    "\n",
    "for img_path, expected_text in sign_images:\n",
    "    process_sign_image(img_path, expected_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
